{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benin Solar Data - Exploratory Data Analysis\n",
    "\n",
    "**Dataset:** benin-malanville.csv  \n",
    "**Objective:** Profile, clean, and explore the Benin solar dataset to identify key trends and insights for solar investment decisions.\n",
    "\n",
    "## Table of Contents\n",
    "1. Setup & Data Loading\n",
    "2. Summary Statistics & Missing Values\n",
    "3. Outlier Detection & Data Quality\n",
    "4. Data Cleaning\n",
    "5. Time Series Analysis\n",
    "6. Cleaning Impact Analysis\n",
    "7. Correlation & Relationship Analysis\n",
    "8. Wind Analysis & Distributions\n",
    "9. Temperature & Humidity Analysis\n",
    "10. Bubble Chart Analysis\n",
    "11. Key Insights & Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Figure size defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/benin-malanville.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data inspection\n",
    "print(\"=\" * 80)\n",
    "print(\"FIRST 5 ROWS:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LAST 5 ROWS:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.tail())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA TYPES:\")\n",
    "print(\"=\" * 80)\n",
    "display(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics & Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for all numeric columns\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS FOR NUMERIC COLUMNS\")\n",
    "print(\"=\" * 80)\n",
    "summary_stats = df.describe()\n",
    "display(summary_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Percentage', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "    \n",
    "    # Flag columns with >5% missing values\n",
    "    high_missing = missing_df[missing_df['Missing Percentage'] > 5]\n",
    "    if len(high_missing) > 0:\n",
    "        print(\"\\n⚠️ Columns with >5% missing values:\")\n",
    "        for col in high_missing['Column']:\n",
    "            print(f\"  - {col}: {high_missing[high_missing['Column']==col]['Missing Percentage'].values[0]:.2f}%\")\n",
    "else:\n",
    "    print(\"✓ No missing values found in the dataset!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection & Data Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative values in solar radiation columns\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY CHECK: NEGATIVE VALUES IN SOLAR RADIATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "solar_cols = ['GHI', 'DNI', 'DHI']\n",
    "for col in solar_cols:\n",
    "    if col in df.columns:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        negative_pct = (negative_count / len(df)) * 100\n",
    "        print(f\"{col}: {negative_count} negative values ({negative_pct:.2f}%)\")\n",
    "        \n",
    "print(\"\\nNote: Negative values during nighttime are expected, but should be investigated for daytime readings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Z-scores for outlier detection\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER DETECTION USING Z-SCORES (|Z| > 3)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Columns to check for outliers\n",
    "outlier_cols = ['GHI', 'DNI', 'DHI', 'ModA', 'ModB', 'WS', 'WSgust']\n",
    "\n",
    "# Create a copy for outlier analysis\n",
    "df_outlier = df.copy()\n",
    "\n",
    "# Store outlier information\n",
    "outlier_summary = []\n",
    "\n",
    "for col in outlier_cols:\n",
    "    if col in df.columns:\n",
    "        # Calculate Z-scores\n",
    "        z_scores = np.abs(stats.zscore(df[col].dropna()))\n",
    "        \n",
    "        # Count outliers (|Z| > 3)\n",
    "        outliers = np.sum(z_scores > 3)\n",
    "        outlier_pct = (outliers / len(df[col].dropna())) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'Outliers': outliers,\n",
    "            'Outlier Percentage': outlier_pct\n",
    "        })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "display(outlier_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned copy of the dataset\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING PROCESS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nOriginal dataset shape: {df_clean.shape}\")\n",
    "\n",
    "# Step 1: Handle missing values in key columns with median imputation\n",
    "key_columns = ['GHI', 'DNI', 'DHI', 'ModA', 'ModB', 'Tamb', 'RH', 'WS', 'WSgust', 'WD', 'BP']\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in df_clean.columns:\n",
    "        missing_before = df_clean[col].isnull().sum()\n",
    "        if missing_before > 0:\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_val, inplace=True)\n",
    "            print(f\"✓ {col}: Imputed {missing_before} missing values with median ({median_val:.2f})\")\n",
    "\n",
    "# Step 2: Remove rows with critical missing data (Comments can be missing)\n",
    "# Only drop rows where all key sensor readings are missing\n",
    "critical_cols = ['GHI', 'DNI', 'DHI']\n",
    "rows_before = len(df_clean)\n",
    "df_clean = df_clean.dropna(subset=critical_cols, how='all')\n",
    "rows_after = len(df_clean)\n",
    "rows_dropped = rows_before - rows_after\n",
    "\n",
    "if rows_dropped > 0:\n",
    "    print(f\"\\n✓ Dropped {rows_dropped} rows with all critical solar radiation values missing\")\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")\n",
    "print(f\"Remaining missing values: {df_clean.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export cleaned dataset\n",
    "output_path = '../data/benin-malanville_clean.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"✓ Cleaned dataset exported to: {output_path}\")\n",
    "print(f\"✓ File size: {len(df_clean)} rows × {len(df_clean.columns)} columns\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
